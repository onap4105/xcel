The stack-kubelet/1 job configuration is not directly defined in the kube-prometheus-stack Helm chart itself. Instead, it is dynamically generated by the Prometheus Operator based on the ServiceMonitor and PodMonitor Custom Resources (CRDs) deployed in your cluster. Here's why you can't find it in the Helm chart:

the container_cpu_usage_seconds_total metric exists (3.82k samples) but lacks Kubernetes labels like namespace, pod, or container, Prometheus is scraping the metric without the proper Kubernetes metadata,  To stop dropping metrics in the stack-kubelet/1 job, update the Helm values.yaml to remove the metric_relabel_configs section, please use the provided values.yaml to make th echange


To stop dropping metrics in the stack-kubelet/1 job, update the Helm values.yaml to remove the metric_relabel_configs section. Here's how to do it:

The kubernetes-cadvisor job is responsible for scraping cAdvisor metrics from the kubelet endpoint, which includes critical labels like namespace, pod, and container. Without this job, Prometheus cannot associate metrics with Kubernetes resources.


I found the root cause and solution for this issue, it is Prometheus scraping the metric without the proper Kubernetes metadata, Kubernetes labels namespace, pod and containermetrics are dropped in the scrape job.
The scrapin gjob configuration is dynamically generated by the Prometheus Operator based on the ServiceMonitor and PodMonitor Custom Resources (CRDs) deployed in the cluster, I will need to update th econfiguration and redeploy it to verify the change. 


The issue occurs because Prometheus is scraping metrics without the proper Kubernetes metadata (e.g., namespace, pod, and container labels), which are being dropped in the scrape job. The job configuration is dynamically generated by the Prometheus Operator using ServiceMonitor and PodMonitor CRDs. To resolve this, I will update the configuration and redeploy it to verify the changes.



